{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"hVLc-i7VIC26"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow\u003c16,\u003e=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePj8aNAqoNtu"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghs7Cn3LIQUa"},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zylT4PwIeGt"},"outputs":[],"source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"Aleemahxmd/GADGET\"\n","# dataset_name = \"Junaidjk/nw\"\n","\n","# Fine-tuned model name\n","new_model = \"/content/drive/MyDrive/mini Project/model3/Llama-2-7b-chat-finetune\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"/content/drive/MyDrive/mini Project/model3\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOagZlWZJG_W"},"outputs":[],"source":["# Load dataset (you can process it here)\n","dataset = load_dataset(dataset_name, split=\"train\")\n","print(dataset)\n","print(type(dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTzd-IegLVKA"},"outputs":[],"source":["import pandas as pd\n","\n","# Sample dataset with potential issues\n","data = {\n","    \"column1\": [1, 2, 3],  # Non-string data\n","    \"column2\": [\"text1\", \"text2\", \"text3\"]  # String data\n","}\n","\n","# Create a DataFrame\n","df = pd.DataFrame(data)\n","\n","# To ensure all columns are strings, convert them\n","df = df.astype(str)\n","\n","# Verify the conversion\n","print(\"Data types:\", df.dtypes)  # Should all be 'object'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LsvX445LZ82"},"outputs":[],"source":["# Load the dataset\n","dataset_name = \"Aleemahxmd/GADGET\"\n","dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Check a sample of the data to identify non-string values\n","print(\"Sample data:\", dataset[:10])\n","\n","# Check data types in the dataset\n","print(\"Data types:\", dataset.features)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltqTT2NfJJif"},"outputs":[],"source":["# Function to convert the 'text' field to a string, with support for batching\n","def convert_to_string(batch):\n","    if 'text' in batch:\n","        # Convert the 'text' field to a list of strings (for batched processing)\n","        batch['text'] = [str(x) for x in batch['text']]\n","    return batch\n","\n","# Apply the conversion function to the dataset\n","dataset = dataset.map(convert_to_string, batched=True)\n","\n","# Verify the conversion by checking a sample of the data\n","print(\"Converted dataset:\", dataset[:10])  # Check first few examples\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDuDL2WYZg93"},"outputs":[],"source":["\n","# dataset = pd.read_csv(\"/content/2-Copy-csv.csv\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major \u003e= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=1025,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY6UdlIKGY8P"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir results/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufEw6nYXGbxz"},"outputs":[],"source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"What is a large language model?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soOX-t3uGdXo"},"outputs":[],"source":["# Empty VRAM\n","del model\n","del pipe\n","del trainer\n","import gc\n","gc.collect()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y10voF2CGfrh"},"outputs":[],"source":["# Reload model in FP16 and merge it with LoRA weights\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhjOFGnRG-ub"},"outputs":[],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CFZ_UlaqHBif"},"outputs":[],"source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"im looking for a gaming laptop in that can play gta 5 .\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0U_hWJYHEdP"},"outputs":[],"source":["# Run text generation pipeline with our next model\n","prompt = \"I need a professional laptop for my child,He is in college \"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=300)\n","result = pipe(f\"[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PA1L1CZhEUf"},"outputs":[],"source":["# Run text generation pipeline with our next model\n","prompt = \"im looking for a laptop in 50-60 k range for college that can perform gaming as well \"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=300)\n","result = pipe(f\"[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgYHPbG9hiUW"},"outputs":[],"source":["# Run text generation pipeline with our next model\n","prompt = \"im looking for specs of dell inspiron 15 3000 series\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=300)\n","result = pipe(f\"[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9IZ_ZKHHHMM"},"outputs":[],"source":["model.save_pretrained(\"Model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMLDYgaMHMNg"},"outputs":[],"source":["# Run text generation pipeline with our next model\n","prompt = \"I need a professional laptop for my child,He is in college . it should also be able to edit videos \"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=300)\n","result = pipe(f\"[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IdveE4_LHOhg"},"outputs":[],"source":["import os\n","os.environ['HF_HUB_TOKEN'] = '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'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YU7KclwNHQzg"},"outputs":[],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6A4afVHHaEW"},"outputs":[],"source":["model.push_to_hub(\"Laptop_Recommendation\"),\n","tokenizer.push_to_hub(\"Laptop_Recommendation\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sy-k8dLUHax2"},"outputs":[],"source":["!pip install tensorflow-gpu\n","!pip install nvidia-tensorrt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hQ29mu-Hevo"},"outputs":[],"source":["tensorboard --logdir=\"/content/drive/MyDrive/mini Project/model3/runs\""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMPMGYXyirXHvAqOgVp/h/J","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}